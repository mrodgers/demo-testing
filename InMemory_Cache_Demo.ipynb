{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMREMTbvjnVLyEDsnesv6NA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrodgers/demo-testing/blob/main/InMemory_Cache_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Cache Techniques Part 1 - InMemory, July 2023\n",
        "Welcome to the PromptMule CacheCast series of Generative AI Cache Demos! This collection of demos is designed to explore various cache techniques for Generative AI models, enabling faster and more efficient inference on large language models. In Part 1, we delve into the realm of \"In-Memory Cache,\" where we'll showcase how caching data in memory can significantly improve the performance of Generative AI models. By employing smart caching strategies, we aim to reduce response times, optimize resource utilization, and create a seamless user experience when interacting with language models. Join us on this exciting journey as we uncover the power of caching in the world of Generative AI!"
      ],
      "metadata": {
        "id": "MUCK7B_889hK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# InMemory Cache Benchmarking Demo\n",
        "\n",
        "This repository contains a Python script for benchmarking the performance of a cache using a sample function (`llm_langchain`) as a test prompt. The script is designed to be executed on Google Colab notebooks.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The Cache Benchmarking Demo is a Python script that measures the execution time of a cache for a specific function (`llm_langchain`) that processes a given text. The script is designed to show the time difference between the first execution (bypassing the cache) and subsequent cache hits.\n",
        "\n",
        "The demo utilizes the `timeit` module to calculate the execution time for each iteration. It provides valuable insights into how caching can significantly improve the performance of repetitive operations.\n",
        "\n",
        "**Note:** Before running the demo, ensure you have an OpenAI API key. Replace `YOUR_OPENAI_API_KEY` in the script with your actual API key.\n",
        "\n",
        "## Usage on Google Colab\n",
        "\n",
        "1. Open Google Colab in your web browser: [https://colab.research.google.com/](https://colab.research.google.com/).\n",
        "\n",
        "2. In Google Colab, click on \"File\" > \"New Notebook\" to create a new notebook.\n",
        "\n",
        "3. In the code cell, paste the following to clone the repository and run the benchmarking demo:\n",
        "\n",
        "```python\n",
        "!git clone https://github.com/mrodgers/demo-testing/blob/main/InMemory_Cache_Demo.ipynb\n",
        "%cd cache-benchmark-demo\n",
        "!pip install openai  # Install OpenAI library\n",
        "!python benchmark.py --api_key=YOUR_OPENAI_API_KEY\n",
        "```\n",
        "\n",
        "4. Replace `your_username` with your actual GitHub username and `YOUR_OPENAI_API_KEY` with your OpenAI API key.\n",
        "\n",
        "5. Click on the \"Run\" button to execute the code. The benchmarking demo will run, and you will see the output showing the execution times.\n",
        "\n",
        "## Functionality\n",
        "\n",
        "### `llm_langchain(text)`\n",
        "\n",
        "The `llm_langchain` function represents a sample prompt processing function. It should be implemented to perform some meaningful processing on the given text and return the result. In this demo, we have implemented a placeholder function that simply returns the input text. Replace this function with your actual prompt processing logic for accurate benchmarking.\n",
        "\n",
        "### Benchmarking\n",
        "\n",
        "The script performs the following steps for benchmarking the cache:\n",
        "\n",
        "1. First Execution (Bypassing Cache):\n",
        "   The script runs the `llm_langchain` function once to measure the time it takes for the first execution without utilizing the cache. This time represents the execution time without cache hits.\n",
        "\n",
        "2. Cache Hits (Subsequent Executions):\n",
        "   After the first execution, the script runs the `llm_langchain` function multiple times (`num_iterations`) to simulate cache hits. The time taken for each execution is measured, and the average execution time is calculated for cache hits.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions to this cache benchmarking demo are welcome! If you find any issues, have suggestions, or want to extend the functionality, feel free to create a pull request.\n",
        "\n",
        "## License\n",
        "\n",
        "The Cache Benchmarking Demo is open-source software licensed under the [MIT License](LICENSE).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tlR4nA6gIhAp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nn0aj4cl8qde"
      },
      "outputs": [],
      "source": [
        "# @title Input OpenAI API Key { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "#@markdown Input your OpenAI API key here. To obtain an OpenAI API key (https://platform.openai.com/account/api-keys), OR sign up on the OpenAI website, provide necessary information, and upon approval, you'll be issued an API key to authenticate your requests to the API.\n",
        "\n",
        "OPENAI_API_KEY = \"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\" #@param {type:\"string\"}\n",
        "#@markdown ---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's test our OpenAI Set up First...\n",
        "- First let's install OpenAI and LangChain Libraries"
      ],
      "metadata": {
        "id": "dpI3lps49F1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RXbPPoS9FiR",
        "outputId": "3d6c46d7-b1ac-47d3-f7a7-1467e973319a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.242)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.14)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Next let's test a basic prompt/response via LangChain..."
      ],
      "metadata": {
        "id": "0UULN9fa9SSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "llm_langchain = OpenAI(model_name=\"text-davinci-003\")\n",
        "text_to_predict = \"Which is the best technical skill to learn in 2023?\"\n",
        "print(llm_langchain(text_to_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSbc94mL9hKR",
        "outputId": "f569d0ce-c6b2-4780-afc6-151d4ddc776a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The best technical skill to learn in 2023 will depend on the individual's career goals and interests. Some popular technical skills that may be useful to learn in 2023 include artificial intelligence, machine learning, data science, blockchain technology, cloud computing, and software development.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Let's see what an in memory cache does..."
      ],
      "metadata": {
        "id": "FGGzHi3H9i59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "# To make the caching really obvious, let's use a slower model.\n",
        "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\n",
        "# Now, let's initialize the in-memory cache\n",
        "langchain.llm_cache = InMemoryCache()"
      ],
      "metadata": {
        "id": "ZTFDybFU9nJH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import timeit\n",
        "\n",
        "# Define the function instruction\n",
        "def instruction():\n",
        "    for i in range(num_iterations):\n",
        "        result = llm_langchain(text_to_predict)  # this is our test prompt function to the llm\n",
        "        print(f\"Iteration #{i + 1}: {result}\")\n",
        "\n",
        "# Perform the first run, bypassing the cache\n",
        "start_time = timeit.timeit(instruction,number=1) # initial execution runs, bypasses cache, should take longest time\n",
        "first_run = start_time\n",
        "\n",
        "# Perform multiple cache hits and average the time\n",
        "num_iterations = 1\n",
        "cache_time = sum(timeit.timeit(instruction, number=1) for _ in range(num_iterations)) / num_iterations\n",
        "\n",
        "print(\"--- WTC Benchmark ---\")\n",
        "print(f\"Time taken for 1st execution: {first_run:.6f} seconds\")\n",
        "print(f\"Time taken for Cache Hit execution (average): {cache_time:.6f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kCy7YoHBaPp",
        "outputId": "c4be74bc-3cca-44a2-b383-5334f7b96437"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration #1: \n",
            "\n",
            "The best technical skill to learn in 2023 is coding. With the increasing use of technology in every industry, coding skills are becoming increasingly important. Coding is the language of the future and it is essential for anyone interested in developing technology-based solutions. With coding, you can create apps, websites, and other digital products. Additionally, coding can help you understand how computers and networks work and can help you create more efficient solutions.\n",
            "Iteration #1: \n",
            "\n",
            "The best technical skill to learn in 2023 is coding. With the increasing use of technology in every industry, coding skills are becoming increasingly important. Coding is the language of the future and it is essential for anyone interested in developing technology-based solutions. With coding, you can create apps, websites, and other digital products. Additionally, coding can help you understand how computers and networks work and can help you create more efficient solutions.\n",
            "--- WTC Benchmark ---\n",
            "Time taken for 1st execution: 0.009934 seconds\n",
            "Time taken for Cache Hit execution (average): 0.001091 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing of the Cache is complete\n",
        "Check out promptmule's cache at www.promptmule.com"
      ],
      "metadata": {
        "id": "EkewyeQtE00c"
      }
    }
  ]
}