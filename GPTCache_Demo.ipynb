{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd4DCRk6EZaXWNAKZLkW3u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrodgers/demo-testing/blob/main/GPTCache_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Cache Techniques Part 3 - GPTCache July 2023\n",
        "\n",
        "Welcome to the PromptMule CacheCast series of Generative AI Cache Demos! This collection of demos is designed to explore various cache techniques for Generative AI models, enabling faster and more efficient inference on large language models. In Part 3, we delve into the realm of \"GPTcache,\" where we'll showcase how caching data using GPTcache can revolutionize the performance of Generative AI-based apps. By harnessing the power of GPTcache, a specialized cache mechanism for language models, we aim to further reduce response times, optimize resource utilization, and deliver an unparalleled user experience when interacting with language models. Join us on this thrilling journey as we unlock the full potential of caching in the world of Generative AI!"
      ],
      "metadata": {
        "id": "V1gu_sIneI94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPTCache Benchmarking Demo with SQLite\n",
        "\n",
        "This repository contains a Python script for benchmarking the performance of GPTCache using an SQLite database as the caching mechanism. The script is designed to be executed on Google Colab notebooks.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The GPTCache Benchmarking Demo with SQLite is a Python script that measures the execution time of GPTCache for a specific function (`llm_langchain`) that processes a given text. The demo showcases how caching data in SQLite using GPTCache can significantly improve the performance of Generative AI models, specifically for semantic search use cases.\n",
        "\n",
        "To use GPTCache with SQLite, we need to initialize the cache and set up the OpenAI API key. The demo demonstrates the initialization process and showcases the usage of GPTCache to process a prompt question and retrieve the response.\n",
        "\n",
        "**Note:** Before running the demo, ensure you have an OpenAI API key. Replace `YOUR_OPENAI_API_KEY` in the script with your actual API key.\n",
        "\n",
        "## Code Explanation\n",
        "\n",
        "The provided Python script performs the following tasks:\n",
        "\n",
        "1. Importing Required Libraries:\n",
        "   The script starts by importing the necessary libraries, including `time` for time measurement.\n",
        "\n",
        "2. `response_text` Function:\n",
        "   The `response_text` function is defined to extract the response text from the OpenAI API response.\n",
        "\n",
        "3. Cache Initialization:\n",
        "   The script imports GPTCache and other related modules, such as `cache` and `openai`. It initializes GPTCache with SQLite as the caching mechanism.\n",
        "\n",
        "4. Prompt Question:\n",
        "   The script defines a prompt question in the `question` variable. This question will be used to evaluate the GPTCache performance.\n",
        "\n",
        "5. Benchmarking:\n",
        "   The script runs a loop twice to simulate two prompt requests. It measures the time it takes to process each prompt using GPTCache. It utilizes the `openai.ChatCompletion.create` method to communicate with the GPT-3.5-turbo model provided by OpenAI.\n",
        "\n",
        "6. Output:\n",
        "   For each prompt request, the script displays the prompt question itself, the time taken for execution, and the generated answer using GPTCache.\n",
        "\n",
        "## Usage on Google Colab\n",
        "\n",
        "1. Open Google Colab in your web browser: [https://colab.research.google.com/](https://colab.research.google.com/).\n",
        "\n",
        "2. In Google Colab, click on \"File\" > \"New Notebook\" to create a new notebook.\n",
        "\n",
        "3. In the code cell, paste the following to clone the repository and run the benchmarking demo:\n",
        "\n",
        "```python\n",
        "!git clone https://github.com/mrodgers/demo-testing/blob/main/GPTCache_SQLite_Demo.ipynb\n",
        "%cd cache-benchmark-demo\n",
        "!pip install openai  # Install OpenAI library\n",
        "!pip install gptcache  # Install GPTCache library\n",
        "!python benchmark.py --api_key=YOUR_OPENAI_API_KEY\n",
        "```\n",
        "\n",
        "4. Replace `your_username` with your actual GitHub username and `YOUR_OPENAI_API_KEY` with your OpenAI API key.\n",
        "\n",
        "5. Click on the \"Run\" button to execute the code. The benchmarking demo will run, and you will see the output showing the execution times.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions to this GPTCache benchmarking demo are welcome! If you find any issues, have suggestions, or want to extend the functionality, feel free to create a pull request.\n",
        "\n",
        "## License\n",
        "\n",
        "The GPTCache Benchmarking Demo with SQLite is open-source software licensed under the [MIT License](LICENSE).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jN3EmjsjflHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Input OpenAI API Key { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "#@markdown Input your OpenAI API key here. To obtain an OpenAI API key (https://platform.openai.com/account/api-keys), OR sign up on the OpenAI website, provide necessary information, and upon approval, you'll be issued an API key to authenticate your requests to the API.\n",
        "\n",
        "OPENAI_API_KEY = \"YOUR_OPENAI_KET_HERE\" #@param {type:\"string\"}\n",
        "#@markdown ---\n"
      ],
      "metadata": {
        "id": "OxmQQ5mJ57YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install GPTCache\n",
        "!pip install gptcache"
      ],
      "metadata": {
        "id": "iKbQyFaN5_G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "llm_langchain = OpenAI(model_name=\"text-davinci-003\")\n",
        "text_to_predict = \"Which is the best technical skill to learn in 2023?\"\n",
        "print(llm_langchain(text_to_predict))"
      ],
      "metadata": {
        "id": "FaByXF236FVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def response_text(openai_resp):\n",
        "    return openai_resp['choices'][0]['message']['content']\n",
        "\n",
        "print(\"Cache loading.....\")\n",
        "\n",
        "# To use GPTCache, that's all you need\n",
        "# -------------------------------------------------\n",
        "from gptcache import cache\n",
        "from gptcache.adapter import openai\n",
        "\n",
        "cache.init()\n",
        "cache.set_openai_key()\n",
        "# -------------------------------------------------\n",
        "\n",
        "question = \"what's github\"\n",
        "for _ in range(2):\n",
        "    start_time = time.time()\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model='gpt-3.5-turbo',\n",
        "      messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': question\n",
        "        }\n",
        "      ],\n",
        "    )\n",
        "    print(f'Question: {question}')\n",
        "    print(\"Time consuming: {:.2f}s\".format(time.time() - start_time))\n",
        "    print(f'Answer: {response_text(response)}\\n')"
      ],
      "metadata": {
        "id": "S9N5O3SZYzyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def response_text(openai_resp):\n",
        "    return openai_resp['choices'][0]['message']['content']\n",
        "\n",
        "from gptcache import cache\n",
        "from gptcache.adapter import openai\n",
        "from gptcache.embedding import Onnx\n",
        "from gptcache.manager import CacheBase, VectorBase, get_data_manager\n",
        "from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation\n",
        "\n",
        "print(\"Cache loading.....\")\n",
        "\n",
        "onnx = Onnx()\n",
        "data_manager = get_data_manager(CacheBase(\"sqlite\"), VectorBase(\"faiss\", dimension=onnx.dimension))\n",
        "cache.init(\n",
        "    embedding_func=onnx.to_embeddings,\n",
        "    data_manager=data_manager,\n",
        "    similarity_evaluation=SearchDistanceEvaluation(),\n",
        "    )\n",
        "cache.set_openai_key()\n",
        "\n",
        "questions = [\n",
        "    \"what's github\",\n",
        "    \"can you explain what GitHub is\",\n",
        "    \"can you tell me more about GitHub\",\n",
        "    \"what is the purpose of GitHub\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    start_time = time.time()\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': question\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    print(f'Question: {question}')\n",
        "    print(\"Time consuming: {:.2f}s\".format(time.time() - start_time))\n",
        "    print(f'Answer: {response_text(response)}\\n')"
      ],
      "metadata": {
        "id": "zEIfvTnLdPyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmMI5NkO5z91"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from gptcache import Cache\n",
        "from gptcache.manager.factory import manager_factory\n",
        "from gptcache.processor.pre import get_prompt\n",
        "from langchain.cache import GPTCache\n",
        "import hashlib\n",
        "def get_hashed_name(name):\n",
        "    return hashlib.sha256(name.encode()).hexdigest()\n",
        "def init_gptcache(cache_obj: Cache, llm: str):\n",
        "    hashed_llm = get_hashed_name(llm)\n",
        "    cache_obj.init(\n",
        "        pre_embedding_func=get_prompt,\n",
        "        data_manager=manager_factory(manager=\"map\", data_dir=f\"map_cache_{hashed_llm}\"),\n",
        "    )\n",
        "langchain.llm_cache = GPTCache(init_gptcache)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import timeit\n",
        "\n",
        "num_iterations = 1\n",
        "# Define the function instruction\n",
        "def instruction():\n",
        "  result = llm_langchain(text_to_predict)  # this is our test prompt function to the llm\n",
        "  print(f\"Response: {result.strip()}\")\n",
        "\n",
        "# Perform the first run, bypassing the cache\n",
        "start_time = timeit.timeit(instruction,number=1) # initial execution runs, bypasses cache, should take longest time\n",
        "first_run = start_time\n",
        "\n",
        "num_iterations = 3\n",
        "# Perform multiple cache hits and average the time\n",
        "cache_time = sum(timeit.timeit(instruction, number=1) for _ in range(num_iterations)) / num_iterations\n",
        "delta = first_run - cache_time\n",
        "\n",
        "print(\"--- WTC Benchmark ---\")\n",
        "print(f\"Time taken for 1st execution: {first_run:.6f} seconds\")\n",
        "print(f\"Time taken for Cache Hit execution (average): {cache_time:.6f} seconds\")\n",
        "print(f\"The delta between Cache hit and OpenAI call is: {delta:.6f} seconds\")"
      ],
      "metadata": {
        "id": "FMvi_ATv6MV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to code examples from:"
      ],
      "metadata": {
        "id": "RRdkkkIsdtvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples found and used in this demo\n",
        "\n",
        "[This example](https://gptcache.readthedocs.io/en/latest/bootcamp/openai/chat.html)\n",
        "[OpenAI Example](https://platform.openai.com/docs/guides/chat/introduction)\n",
        "\n",
        "Visit www.promptmule.com"
      ],
      "metadata": {
        "id": "j0glnMd8isnE"
      }
    }
  ]
}