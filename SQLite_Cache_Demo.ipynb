{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPzZSUl7JSnvzk8mm+vA3Cg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrodgers/demo-testing/blob/main/SQLite_Cache_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Cache Techniques Part 2 - , SQLite July 2023\n",
        "Welcome to the PromptMule CacheCast series of Generative AI Cache Demos! This collection of demos is designed to explore various cache techniques for Generative AI models, enabling faster and more efficient inference on large language models. In Part 2, we delve into the realm of \"SQLite Cache,\" where we'll showcase how caching data in SQLite can significantly improve the performance of Generative AI-based apps. By employing smart caching strategies, we aim to reduce response times, optimize resource utilization, and create a seamless user experience when interacting with language models. Join us on this exciting journey as we uncover the power of caching in the world of Generative AI!"
      ],
      "metadata": {
        "id": "q3w-EZQqOKPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1 - input your OpenAI API Key here."
      ],
      "metadata": {
        "id": "Pazc3VPhO0wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Input OpenAI API Key { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "#@markdown Input your OpenAI API key here. To obtain an OpenAI API key (https://platform.openai.com/account/api-keys), OR sign up on the OpenAI website, provide necessary information, and upon approval, you'll be issued an API key to authenticate your requests to the API.\n",
        "\n",
        "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\" #@param {type:\"string\"}\n",
        "#@markdown ---\n"
      ],
      "metadata": {
        "id": "OFWTU8H_OlyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's test our OpenAI Set up First...\n",
        "- First let's install OpenAI and LangChain Libraries"
      ],
      "metadata": {
        "id": "gZgxXpCbO-GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "nSgbe8yaPCpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Next let's test a basic prompt/response via LangChain...\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WO2Sxr0zPFa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "llm_langchain = OpenAI(model_name=\"text-davinci-003\")\n",
        "text_to_predict = \"Which is the best technical skill to learn in 2023?\"\n",
        "print(llm_langchain(text_to_predict))"
      ],
      "metadata": {
        "id": "_O6lPl12PHmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "# Let's see what an in memory cache does..."
      ],
      "metadata": {
        "id": "HrNp8yVLPTGH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzcbAiXSN9Px"
      },
      "outputs": [],
      "source": [
        "from langchain.cache import SQLiteCache\n",
        "import langchain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "# To make the caching really obvious, let's use a slower model.\n",
        "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\n",
        "# We can do the same thing with a SQLite cache\n",
        "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative AI Cache Wall Time Clock (WTC) Benchmark\n",
        "\n",
        "The benchmark code aims to test the performance of the Generative AI cache technique using a sample function called `llm_langchain`. The code utilizes the `timeit` module to measure the execution time of the `llm_langchain` function with and without cache hits.\n",
        "\n",
        "### Code Overview\n",
        "\n",
        "```python\n",
        "import time\n",
        "import timeit\n",
        "\n",
        "# Define the function instruction\n",
        "def instruction():\n",
        "    for i in range(num_iterations):\n",
        "        result = llm_langchain(text_to_predict)  # this is our test prompt function to the llm\n",
        "        print(f\"Iteration #{i + 1}: {result}\")\n",
        "\n",
        "# Perform the first run, bypassing the cache\n",
        "start_time = timeit.timeit(instruction,number=1) # initial execution runs, bypasses cache, should take longest time\n",
        "first_run = start_time\n",
        "\n",
        "# Perform multiple cache hits and average the time\n",
        "num_iterations = 1\n",
        "cache_time = sum(timeit.timeit(instruction, number=1) for _ in range(num_iterations)) / num_iterations\n",
        "\n",
        "print(\"--- WTC Benchmark ---\")\n",
        "print(f\"Time taken for 1st execution: {first_run:.6f} seconds\")\n",
        "print(f\"Time taken for Cache Hit execution (average): {cache_time:.6f} seconds\")\n",
        "\n",
        "The instruction function simulates multiple cache hits by running the llm_langchain function in a loop for a specified number of iterations (num_iterations). The first execution bypasses the cache, allowing for the measurement of the initial response time. Subsequently, the code runs the function multiple times to imitate cache hits, calculating the average execution time.\n",
        "\n",
        "The output provides insightful information about the time differences between the first execution and the subsequent cache hits, providing valuable metrics for assessing the cache technique's efficiency. Additionally, the program labels the execution times and displays the results neatly with clear identifiers, making it easy to interpret and analyze the benchmark data."
      ],
      "metadata": {
        "id": "6m8p17PVPdmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import timeit\n",
        "\n",
        "# Define the function instruction\n",
        "def instruction():\n",
        "    for i in range(num_iterations):\n",
        "        result = llm_langchain(text_to_predict)  # this is our test prompt function to the llm\n",
        "        print(f\"Iteration #{i + 1}: {result}\")\n",
        "\n",
        "# Perform the first run, bypassing the cache\n",
        "start_time = timeit.timeit(instruction,number=1) # initial execution runs, bypasses cache, should take longest time\n",
        "first_run = start_time\n",
        "\n",
        "# Perform multiple cache hits and average the time\n",
        "num_iterations = 1\n",
        "cache_time = sum(timeit.timeit(instruction, number=1) for _ in range(num_iterations)) / num_iterations\n",
        "\n",
        "print(\"--- WTC Benchmark ---\")\n",
        "print(f\"Time taken for 1st execution: {first_run:.6f} seconds\")\n",
        "print(f\"Time taken for Cache Hit execution (average): {cache_time:.6f} seconds\")"
      ],
      "metadata": {
        "id": "SV2aoCb7PctJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing of the Cache is complete\n",
        "Look for additional Parts on out Github @promptmule or @promptmule4real or our youtube channel @cachecast\n",
        "\n",
        "Check out promptmule's cloud semantic cache at www.promptmule.com"
      ],
      "metadata": {
        "id": "AuZlWE4gPliZ"
      }
    }
  ]
}